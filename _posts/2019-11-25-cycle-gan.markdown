---
layout: default
title:  "CycleGAN日记"
date:   2019-11-25 19:00:00
categories: network
---

# CycleGAN

CycleGAN 是一种Gan（生成式对抗网络）。用于对两个成对图片域A和域B间进行映射。

文章：https://arxiv.org/abs/1703.10593

# CycleGAN理论

图片源域：$X$, 图片目标域$Y$

正映射$G$：$G:X \rightarrow Y$, 负映射$G$：$F:Y \rightarrow X$

## 该模型由两个判别器和生成器构成。

两个判别器可标记为 $D_X$, $D_Y$，分别对图片是否属于A或者B进行判断

两个生成去可标记为 $G$， $F$，分别为从$X$到$Y$和从$Y$到$X$的映射

## 目标函数

$L_{GAN}(F, D_X, Y, X)=E_{x \sim p_{data}(x)}[\log D_X(x)]+E_{y\sim p_{data}(y)}[\log (1-D_X(F(y)))]$

$L_{GAN}(G, D_Y, X, Y)=E_{y \sim p_{data}(y)}[\log D_Y(y)]+E_{x\sim p_{data}(x)}[\log (1-D_Y(G(x)))]$

$L_{cyc}(G, F)=E_{y\sim p_{data}(y)}[||F(G(x))-x||]+E_{y\sim p_{data}(y)}[||G(F(y))-y||]$

$L(G, F, D_X, D_Y) =L_{GAN}(G, D_Y, X, Y) + L_{GAN}(F, D_X, Y, X)+ \lambda L_{cyc}(G, F)$

$G^*, F^*=arg\,\min_{G,F} \max_{D_X,D_Y} L(G, F, D_X, D_Y)$

# CycleGAN实现

## CycleGAN判别器

``` python
num_channels_in, num_discriminator_filter, max_layers=3, use_sigmoid=True
input_a = Input(shape=(None, None, num_channels_in))
t = input_a
t = Conv2D(num_discriminator_filter, kernel_size=4, strides=2, padding="same", name='First') (t)
t = LeakyReLU(alpha=0.2)(t)

for layer in range(1, max_layers):
    out_feat = num_discriminator_filter * min(2**layer, 8)
    t = Conv2D(out_feat, kernel_size=4, strides=2, padding="same", 
                use_bias=False, name='pyramid.{0}'.format(layer)
                )(t)
    t = batchnorm()(t, training=1)
    t = LeakyReLU(alpha=0.2)(t)

out_feat = num_discriminator_filter*min(2**max_layers, 8)
t = Conv2D(out_feat, kernel_size=4, padding='same', use_bias=False, name='pyramid_last')(t)
t = batchnorm()(t, training=1)
t = LeakyReLU(alpha=0.2)(t)

# final layer
t = Conv2D(1, kernel_size=4, name='final'.format(out_feat, 1), padding='same',
            activation="sigmoid" if use_sigmoid else None
            )(t)    
return Model(inputs=[input_a], outputs=t)
```

## CycleGAN生成器


``` python
isize, num_channel_in=3, num_channel_out=3, num_generator_filter=64, fixed_input_size=True
max_num_filter = 8 * num_generator_filter

def block(x, size, num_filter_in, use_batchnorm=True, num_filter_out=None, num_filter_next=None):
    assert size >= 2 and size % 2 == 0
    if num_filter_next is None:
        num_filter_next = min(num_filter_in*2, max_num_filter)
    if num_filter_out is None:
        num_filter_out = num_filter_in
    x = Conv2D(num_filter_next, kernel_size=4, strides=2, use_bias=(not (use_batchnorm and size > 2)),
                padding='same', name='conv_{0}'.format(size)
                )(x)
    if size > 2:
        if use_batchnorm:
            x = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(x, training=1)
        x2 = LeakyReLU(alpha=0.2)(x)
        x2 = block(x2, size//2, num_filter_next)
        x = Concatenate()([x, x2])            
    x = Activation("relu")(x)
    x = Conv2DTranspose(num_filter_out, kernel_size=4, strides=2, use_bias=not use_batchnorm,
                        name='convt.{0}'.format(size))(x)        
    x = Cropping2D(1)(x)
    if use_batchnorm:
        x = BatchNormalization(momentum=0.9, epsilon=1.01e-5)(x, training=1)
    if size <= 8:
        x = Dropout(0.5)(x, training=1)
    return x

size = isize if fixed_input_size else None
t = inputs = Input(shape=(size, size, num_channel_in))        
t = block(t, isize, num_channel_in, False, num_filter_out=num_channel_out, num_filter_next=num_generator_filter)
t = Activation('tanh')(t)
return Model(inputs=inputs, outputs=[t])
```
